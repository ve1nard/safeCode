03/31/2025 15:14:43 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 16:25:30 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 16:53:04 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 18:28:55 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 18:35:51 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 19:16:55 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 19:39:32 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 19:52:07 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 19:53:45 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 19:54:27 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 23:07:08 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 23:10:18 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 23:15:15 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 23:24:00 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 23:28:48 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 23:33:54 - INFO - root -   Device: cuda, n_gpu: 2
03/31/2025 23:52:16 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 15:22:53 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 15:26:53 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 15:33:23 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 15:33:38 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/02/2025 15:33:38 - INFO - root -   ***** Running training *****
04/02/2025 15:33:38 - INFO - root -     Num samples = 1087
04/02/2025 15:33:38 - INFO - root -     Num epoch = 5
04/02/2025 15:33:38 - INFO - root -     Batch size= 1
04/02/2025 15:33:38 - INFO - root -     Total batch size (w. accumulation) = 2
04/02/2025 15:33:38 - INFO - root -     Gradient Accumulation steps = 2
04/02/2025 15:33:38 - INFO - root -     Total optimization steps = 2715
04/02/2025 15:33:38 - INFO - root -     Num val samples = 119
04/02/2025 15:33:38 - INFO - root -     Num parameters = 1777875968
04/02/2025 15:33:38 - INFO - root -     Num trainable parameters = 1204224
04/02/2025 15:33:38 - INFO - root -     Fraction of trainable parameters = 0.0677
04/02/2025 15:44:24 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 15:44:41 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/02/2025 15:44:41 - INFO - root -   ***** Running training *****
04/02/2025 15:44:41 - INFO - root -     Num samples = 1087
04/02/2025 15:44:41 - INFO - root -     Num epoch = 5
04/02/2025 15:44:41 - INFO - root -     Batch size= 1
04/02/2025 15:44:41 - INFO - root -     Total batch size (w. accumulation) = 2
04/02/2025 15:44:41 - INFO - root -     Gradient Accumulation steps = 2
04/02/2025 15:44:41 - INFO - root -     Total optimization steps = 2715
04/02/2025 15:44:41 - INFO - root -     Num val samples = 119
04/02/2025 15:44:41 - INFO - root -     Num parameters = 1777875968
04/02/2025 15:44:41 - INFO - root -     Num trainable parameters = 1204224
04/02/2025 15:44:41 - INFO - root -     Fraction of trainable parameters = 0.0677
04/02/2025 17:01:15 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 17:01:32 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/02/2025 17:01:32 - INFO - root -   ***** Running training *****
04/02/2025 17:01:32 - INFO - root -     Num samples = 1087
04/02/2025 17:01:32 - INFO - root -     Num epoch = 5
04/02/2025 17:01:32 - INFO - root -     Batch size= 1
04/02/2025 17:01:32 - INFO - root -     Total batch size (w. accumulation) = 2
04/02/2025 17:01:32 - INFO - root -     Gradient Accumulation steps = 2
04/02/2025 17:01:32 - INFO - root -     Total optimization steps = 2715
04/02/2025 17:01:32 - INFO - root -     Num val samples = 119
04/02/2025 17:01:32 - INFO - root -     Num parameters = 1777875968
04/02/2025 17:01:32 - INFO - root -     Num trainable parameters = 1204224
04/02/2025 17:01:32 - INFO - root -     Fraction of trainable parameters = 0.0677
04/02/2025 17:07:15 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 17:07:30 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/02/2025 17:07:30 - INFO - root -   ***** Running training *****
04/02/2025 17:07:30 - INFO - root -     Num samples = 1087
04/02/2025 17:07:30 - INFO - root -     Num epoch = 5
04/02/2025 17:07:30 - INFO - root -     Batch size= 1
04/02/2025 17:07:30 - INFO - root -     Total batch size (w. accumulation) = 2
04/02/2025 17:07:30 - INFO - root -     Gradient Accumulation steps = 2
04/02/2025 17:07:30 - INFO - root -     Total optimization steps = 2715
04/02/2025 17:07:30 - INFO - root -     Num val samples = 119
04/02/2025 17:07:30 - INFO - root -     Num parameters = 1777875968
04/02/2025 17:07:30 - INFO - root -     Num trainable parameters = 1204224
04/02/2025 17:07:30 - INFO - root -     Fraction of trainable parameters = 0.0677
04/02/2025 17:18:46 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 17:19:01 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/02/2025 17:19:01 - INFO - root -   ***** Running training *****
04/02/2025 17:19:01 - INFO - root -     Num samples = 1087
04/02/2025 17:19:01 - INFO - root -     Num epoch = 5
04/02/2025 17:19:01 - INFO - root -     Batch size= 1
04/02/2025 17:19:01 - INFO - root -     Total batch size (w. accumulation) = 2
04/02/2025 17:19:01 - INFO - root -     Gradient Accumulation steps = 2
04/02/2025 17:19:01 - INFO - root -     Total optimization steps = 2715
04/02/2025 17:19:01 - INFO - root -     Num val samples = 119
04/02/2025 17:19:01 - INFO - root -     Num parameters = 1777875968
04/02/2025 17:19:01 - INFO - root -     Num trainable parameters = 1204224
04/02/2025 17:19:01 - INFO - root -     Fraction of trainable parameters = 0.0677
04/02/2025 17:24:46 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 17:25:02 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/02/2025 17:25:02 - INFO - root -   ***** Running training *****
04/02/2025 17:25:02 - INFO - root -     Num samples = 1087
04/02/2025 17:25:02 - INFO - root -     Num epoch = 5
04/02/2025 17:25:02 - INFO - root -     Batch size= 1
04/02/2025 17:25:02 - INFO - root -     Total batch size (w. accumulation) = 2
04/02/2025 17:25:02 - INFO - root -     Gradient Accumulation steps = 2
04/02/2025 17:25:02 - INFO - root -     Total optimization steps = 2715
04/02/2025 17:25:02 - INFO - root -     Num val samples = 119
04/02/2025 17:25:02 - INFO - root -     Num parameters = 1777875968
04/02/2025 17:25:02 - INFO - root -     Num trainable parameters = 1204224
04/02/2025 17:25:02 - INFO - root -     Fraction of trainable parameters = 0.0677
04/02/2025 17:28:46 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 17:29:01 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/02/2025 17:29:01 - INFO - root -   ***** Running training *****
04/02/2025 17:29:01 - INFO - root -     Num samples = 1087
04/02/2025 17:29:01 - INFO - root -     Num epoch = 5
04/02/2025 17:29:01 - INFO - root -     Batch size= 1
04/02/2025 17:29:01 - INFO - root -     Total batch size (w. accumulation) = 2
04/02/2025 17:29:01 - INFO - root -     Gradient Accumulation steps = 2
04/02/2025 17:29:01 - INFO - root -     Total optimization steps = 2715
04/02/2025 17:29:01 - INFO - root -     Num val samples = 119
04/02/2025 17:29:01 - INFO - root -     Num parameters = 1777875968
04/02/2025 17:29:01 - INFO - root -     Num trainable parameters = 1204224
04/02/2025 17:29:01 - INFO - root -     Fraction of trainable parameters = 0.0677
04/02/2025 17:36:46 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 17:37:03 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/02/2025 17:37:03 - INFO - root -   ***** Running training *****
04/02/2025 17:37:03 - INFO - root -     Num samples = 1087
04/02/2025 17:37:03 - INFO - root -     Num epoch = 5
04/02/2025 17:37:03 - INFO - root -     Batch size= 1
04/02/2025 17:37:03 - INFO - root -     Total batch size (w. accumulation) = 2
04/02/2025 17:37:03 - INFO - root -     Gradient Accumulation steps = 2
04/02/2025 17:37:03 - INFO - root -     Total optimization steps = 2715
04/02/2025 17:37:03 - INFO - root -     Num val samples = 119
04/02/2025 17:37:03 - INFO - root -     Num parameters = 1777875968
04/02/2025 17:37:03 - INFO - root -     Num trainable parameters = 1204224
04/02/2025 17:37:03 - INFO - root -     Fraction of trainable parameters = 0.0677
04/02/2025 17:46:47 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 17:47:02 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/02/2025 17:47:02 - INFO - root -   ***** Running training *****
04/02/2025 17:47:02 - INFO - root -     Num samples = 1087
04/02/2025 17:47:02 - INFO - root -     Num epoch = 5
04/02/2025 17:47:02 - INFO - root -     Batch size= 1
04/02/2025 17:47:02 - INFO - root -     Total batch size (w. accumulation) = 2
04/02/2025 17:47:02 - INFO - root -     Gradient Accumulation steps = 2
04/02/2025 17:47:02 - INFO - root -     Total optimization steps = 2715
04/02/2025 17:47:02 - INFO - root -     Num val samples = 119
04/02/2025 17:47:02 - INFO - root -     Num parameters = 1777875968
04/02/2025 17:47:02 - INFO - root -     Num trainable parameters = 1204224
04/02/2025 17:47:02 - INFO - root -     Fraction of trainable parameters = 0.0677
04/02/2025 18:55:21 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 18:55:36 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/02/2025 18:55:36 - INFO - root -   ***** Running training *****
04/02/2025 18:55:36 - INFO - root -     Num samples = 1087
04/02/2025 18:55:36 - INFO - root -     Num epoch = 5
04/02/2025 18:55:36 - INFO - root -     Batch size= 1
04/02/2025 18:55:36 - INFO - root -     Total batch size (w. accumulation) = 2
04/02/2025 18:55:36 - INFO - root -     Gradient Accumulation steps = 2
04/02/2025 18:55:36 - INFO - root -     Total optimization steps = 2715
04/02/2025 18:55:36 - INFO - root -     Num val samples = 119
04/02/2025 18:55:36 - INFO - root -     Num parameters = 1777875968
04/02/2025 18:55:36 - INFO - root -     Num trainable parameters = 1204224
04/02/2025 18:55:36 - INFO - root -     Fraction of trainable parameters = 0.0677
04/02/2025 18:58:22 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 18:58:35 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/02/2025 18:58:35 - INFO - root -   ***** Running training *****
04/02/2025 18:58:35 - INFO - root -     Num samples = 1087
04/02/2025 18:58:35 - INFO - root -     Num epoch = 5
04/02/2025 18:58:35 - INFO - root -     Batch size= 1
04/02/2025 18:58:35 - INFO - root -     Total batch size (w. accumulation) = 2
04/02/2025 18:58:35 - INFO - root -     Gradient Accumulation steps = 2
04/02/2025 18:58:35 - INFO - root -     Total optimization steps = 2715
04/02/2025 18:58:35 - INFO - root -     Num val samples = 119
04/02/2025 18:58:35 - INFO - root -     Num parameters = 1777875968
04/02/2025 18:58:35 - INFO - root -     Num trainable parameters = 1204224
04/02/2025 18:58:35 - INFO - root -     Fraction of trainable parameters = 0.0677
04/02/2025 21:28:35 - INFO - root -   Device: cuda, n_gpu: 2
04/02/2025 21:28:52 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/02/2025 21:28:52 - INFO - root -   ***** Running training *****
04/02/2025 21:28:52 - INFO - root -     Num samples = 1087
04/02/2025 21:28:52 - INFO - root -     Num epoch = 5
04/02/2025 21:28:52 - INFO - root -     Batch size= 1
04/02/2025 21:28:52 - INFO - root -     Total batch size (w. accumulation) = 2
04/02/2025 21:28:52 - INFO - root -     Gradient Accumulation steps = 2
04/02/2025 21:28:52 - INFO - root -     Total optimization steps = 2715
04/02/2025 21:28:52 - INFO - root -     Num val samples = 119
04/02/2025 21:28:52 - INFO - root -     Num parameters = 1777875968
04/02/2025 21:28:52 - INFO - root -     Num trainable parameters = 1204224
04/02/2025 21:28:52 - INFO - root -     Fraction of trainable parameters = 0.0677
04/02/2025 21:30:54 - INFO - root -   epochs: 1/5, steps: 100/2715, lm_loss: 14.4693, contrastive_loss: 3.0276, kl_loss: 5.59898965, loss: 23.096
04/02/2025 21:32:50 - INFO - root -   epochs: 1/5, steps: 200/2715, lm_loss: 13.8215, contrastive_loss: 2.753, kl_loss: 4.75484706, loss: 21.3293
04/02/2025 21:34:50 - INFO - root -   epochs: 1/5, steps: 300/2715, lm_loss: 13.4778, contrastive_loss: 2.6154, kl_loss: 4.5245187, loss: 20.6177
04/02/2025 21:36:43 - INFO - root -   epochs: 1/5, steps: 400/2715, lm_loss: 13.2546, contrastive_loss: 2.5512, kl_loss: 4.48294516, loss: 20.2887
04/02/2025 21:38:38 - INFO - root -   epochs: 1/5, steps: 500/2715, lm_loss: 12.9927, contrastive_loss: 2.5846, kl_loss: 4.4165475, loss: 19.9938
04/02/2025 21:40:04 - INFO - root -   val epoch 1: lm_loss: 12.3617, contrastive_loss: 2.1488, kl_loss: 4.31034223, loss: 18.8209
04/02/2025 21:40:04 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-1 and ../trained/qwen-prefix-new/checkpoint-last
04/02/2025 21:41:13 - INFO - root -   epochs: 2/5, steps: 600/2715, lm_loss: 12.8323, contrastive_loss: 2.3686, kl_loss: 4.42840941, loss: 19.6292
04/02/2025 21:42:56 - INFO - root -   epochs: 2/5, steps: 700/2715, lm_loss: 11.9499, contrastive_loss: 1.918, kl_loss: 4.43583, loss: 18.3038
04/02/2025 21:44:55 - INFO - root -   epochs: 2/5, steps: 800/2715, lm_loss: 12.0676, contrastive_loss: 2.207, kl_loss: 4.29975834, loss: 18.5743
04/02/2025 21:46:56 - INFO - root -   epochs: 2/5, steps: 900/2715, lm_loss: 11.6234, contrastive_loss: 2.1695, kl_loss: 4.33073549, loss: 18.1236
04/02/2025 21:48:58 - INFO - root -   epochs: 2/5, steps: 1000/2715, lm_loss: 11.6712, contrastive_loss: 2.1251, kl_loss: 4.26911497, loss: 18.0655
04/02/2025 21:51:16 - INFO - root -   val epoch 2: lm_loss: 11.2176, contrastive_loss: 2.037, kl_loss: 4.05718495, loss: 17.3118
04/02/2025 21:51:16 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-2 and ../trained/qwen-prefix-new/checkpoint-last
04/02/2025 21:51:33 - INFO - root -   epochs: 3/5, steps: 1100/2715, lm_loss: 11.8565, contrastive_loss: 2.2078, kl_loss: 4.23481837, loss: 18.2991
04/02/2025 21:53:27 - INFO - root -   epochs: 3/5, steps: 1200/2715, lm_loss: 10.8259, contrastive_loss: 1.5051, kl_loss: 4.33221017, loss: 16.6632
04/02/2025 21:55:29 - INFO - root -   epochs: 3/5, steps: 1300/2715, lm_loss: 10.8867, contrastive_loss: 1.628, kl_loss: 4.18347677, loss: 16.6982
04/02/2025 21:57:21 - INFO - root -   epochs: 3/5, steps: 1400/2715, lm_loss: 10.9082, contrastive_loss: 1.6692, kl_loss: 4.23559825, loss: 16.813
04/02/2025 21:59:13 - INFO - root -   epochs: 3/5, steps: 1500/2715, lm_loss: 10.961, contrastive_loss: 1.8105, kl_loss: 4.07406549, loss: 16.8455
04/02/2025 22:01:11 - INFO - root -   epochs: 3/5, steps: 1600/2715, lm_loss: 10.8643, contrastive_loss: 1.7114, kl_loss: 4.08731048, loss: 16.663
04/02/2025 22:02:27 - INFO - root -   val epoch 3: lm_loss: 10.1831, contrastive_loss: 2.1641, kl_loss: 4.21714768, loss: 16.5643
04/02/2025 22:02:27 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-3 and ../trained/qwen-prefix-new/checkpoint-last
04/02/2025 22:03:52 - INFO - root -   epochs: 4/5, steps: 1700/2715, lm_loss: 10.7135, contrastive_loss: 1.5067, kl_loss: 4.08412236, loss: 16.3043
04/02/2025 22:05:52 - INFO - root -   epochs: 4/5, steps: 1800/2715, lm_loss: 10.2922, contrastive_loss: 1.2843, kl_loss: 4.11241336, loss: 15.6889
04/02/2025 22:07:50 - INFO - root -   epochs: 4/5, steps: 1900/2715, lm_loss: 10.1856, contrastive_loss: 1.3273, kl_loss: 4.06737341, loss: 15.5803
04/02/2025 22:09:47 - INFO - root -   epochs: 4/5, steps: 2000/2715, lm_loss: 10.5599, contrastive_loss: 1.3551, kl_loss: 4.09755373, loss: 16.0126
04/02/2025 22:11:42 - INFO - root -   epochs: 4/5, steps: 2100/2715, lm_loss: 9.995, contrastive_loss: 1.3743, kl_loss: 4.06519768, loss: 15.4345
04/02/2025 22:13:38 - INFO - root -   val epoch 4: lm_loss: 10.109, contrastive_loss: 2.0232, kl_loss: 4.0577757, loss: 16.1899
04/02/2025 22:13:38 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-4 and ../trained/qwen-prefix-new/checkpoint-last
04/02/2025 22:14:15 - INFO - root -   epochs: 5/5, steps: 2200/2715, lm_loss: 10.2354, contrastive_loss: 1.3527, kl_loss: 4.01917988, loss: 15.6073
04/02/2025 22:16:05 - INFO - root -   epochs: 5/5, steps: 2300/2715, lm_loss: 9.798, contrastive_loss: 0.9914, kl_loss: 4.07317747, loss: 14.8626
04/02/2025 22:18:04 - INFO - root -   epochs: 5/5, steps: 2400/2715, lm_loss: 9.6993, contrastive_loss: 1.1059, kl_loss: 3.99513722, loss: 14.8003
04/02/2025 22:19:59 - INFO - root -   epochs: 5/5, steps: 2500/2715, lm_loss: 9.9186, contrastive_loss: 1.0392, kl_loss: 4.04479078, loss: 15.0025
04/02/2025 22:22:01 - INFO - root -   epochs: 5/5, steps: 2600/2715, lm_loss: 10.1962, contrastive_loss: 1.2085, kl_loss: 3.9583244, loss: 15.363
04/02/2025 22:23:57 - INFO - root -   epochs: 5/5, steps: 2700/2715, lm_loss: 9.933, contrastive_loss: 1.1026, kl_loss: 4.05068408, loss: 15.0863
04/02/2025 22:24:50 - INFO - root -   val epoch 5: lm_loss: 9.8009, contrastive_loss: 1.9348, kl_loss: 3.9785717, loss: 15.7143
04/02/2025 22:24:50 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-5 and ../trained/qwen-prefix-new/checkpoint-last
04/18/2025 00:22:21 - INFO - root -   Device: cuda, n_gpu: 2
04/18/2025 00:22:43 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/18/2025 00:22:43 - INFO - root -   ***** Running training *****
04/18/2025 00:22:43 - INFO - root -     Num samples = 885
04/18/2025 00:22:43 - INFO - root -     Num epoch = 5
04/18/2025 00:22:43 - INFO - root -     Batch size= 1
04/18/2025 00:22:43 - INFO - root -     Total batch size (w. accumulation) = 2
04/18/2025 00:22:43 - INFO - root -     Gradient Accumulation steps = 2
04/18/2025 00:22:43 - INFO - root -     Total optimization steps = 2210
04/18/2025 00:22:43 - INFO - root -     Num val samples = 99
04/18/2025 00:22:43 - INFO - root -     Num parameters = 1777875968
04/18/2025 00:22:43 - INFO - root -     Num trainable parameters = 1204224
04/18/2025 00:22:43 - INFO - root -     Fraction of trainable parameters = 0.0677
04/18/2025 00:25:24 - INFO - root -   epochs: 1/5, steps: 100/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 00:27:55 - INFO - root -   epochs: 1/5, steps: 200/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 00:30:39 - INFO - root -   epochs: 1/5, steps: 300/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 00:33:22 - INFO - root -   epochs: 1/5, steps: 400/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 00:35:17 - INFO - root -   val epoch 1: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 00:35:17 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-1 and ../trained/qwen-prefix-new/checkpoint-last
04/18/2025 00:36:46 - INFO - root -   epochs: 2/5, steps: 500/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 00:39:36 - INFO - root -   epochs: 2/5, steps: 600/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 00:42:10 - INFO - root -   epochs: 2/5, steps: 700/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 00:44:52 - INFO - root -   epochs: 2/5, steps: 800/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 00:47:45 - INFO - root -   val epoch 2: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 00:47:45 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-2 and ../trained/qwen-prefix-new/checkpoint-last
04/18/2025 00:48:10 - INFO - root -   epochs: 3/5, steps: 900/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 00:50:47 - INFO - root -   epochs: 3/5, steps: 1000/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 00:53:26 - INFO - root -   epochs: 3/5, steps: 1100/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 00:56:13 - INFO - root -   epochs: 3/5, steps: 1200/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 00:58:52 - INFO - root -   epochs: 3/5, steps: 1300/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 01:00:13 - INFO - root -   val epoch 3: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 01:00:13 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-3 and ../trained/qwen-prefix-new/checkpoint-last
04/18/2025 01:02:09 - INFO - root -   epochs: 4/5, steps: 1400/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 01:04:45 - INFO - root -   epochs: 4/5, steps: 1500/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 01:07:41 - INFO - root -   epochs: 4/5, steps: 1600/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 01:10:16 - INFO - root -   epochs: 4/5, steps: 1700/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 01:12:42 - INFO - root -   val epoch 4: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 01:12:42 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-4 and ../trained/qwen-prefix-new/checkpoint-last
04/18/2025 01:13:30 - INFO - root -   epochs: 5/5, steps: 1800/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 01:16:05 - INFO - root -   epochs: 5/5, steps: 1900/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 01:18:47 - INFO - root -   epochs: 5/5, steps: 2000/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 01:21:31 - INFO - root -   epochs: 5/5, steps: 2100/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 01:24:16 - INFO - root -   epochs: 5/5, steps: 2200/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 01:25:17 - INFO - root -   val epoch 5: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 01:25:17 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-5 and ../trained/qwen-prefix-new/checkpoint-last
04/18/2025 06:57:14 - INFO - root -   Device: cuda, n_gpu: 2
04/18/2025 06:57:27 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/18/2025 06:57:27 - INFO - root -   ***** Running training *****
04/18/2025 06:57:27 - INFO - root -     Num samples = 1079
04/18/2025 06:57:27 - INFO - root -     Num epoch = 5
04/18/2025 06:57:27 - INFO - root -     Batch size= 1
04/18/2025 06:57:27 - INFO - root -     Total batch size (w. accumulation) = 2
04/18/2025 06:57:27 - INFO - root -     Gradient Accumulation steps = 2
04/18/2025 06:57:27 - INFO - root -     Total optimization steps = 2695
04/18/2025 06:57:27 - INFO - root -     Num val samples = 119
04/18/2025 06:57:27 - INFO - root -     Num parameters = 1777875968
04/18/2025 06:57:27 - INFO - root -     Num trainable parameters = 1204224
04/18/2025 06:57:27 - INFO - root -     Fraction of trainable parameters = 0.0677
04/18/2025 06:59:43 - INFO - root -   epochs: 1/5, steps: 100/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:01:45 - INFO - root -   epochs: 1/5, steps: 200/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:03:50 - INFO - root -   epochs: 1/5, steps: 300/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:05:51 - INFO - root -   epochs: 1/5, steps: 400/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:07:50 - INFO - root -   epochs: 1/5, steps: 500/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:09:16 - INFO - root -   val epoch 1: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:09:16 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-1 and ../trained/qwen-prefix-new/checkpoint-last
04/18/2025 07:10:36 - INFO - root -   epochs: 2/5, steps: 600/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:12:46 - INFO - root -   epochs: 2/5, steps: 700/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:14:46 - INFO - root -   epochs: 2/5, steps: 800/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:16:46 - INFO - root -   epochs: 2/5, steps: 900/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:18:51 - INFO - root -   epochs: 2/5, steps: 1000/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:21:05 - INFO - root -   val epoch 2: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:21:05 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-2 and ../trained/qwen-prefix-new/checkpoint-last
04/18/2025 07:21:30 - INFO - root -   epochs: 3/5, steps: 1100/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:23:41 - INFO - root -   epochs: 3/5, steps: 1200/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:25:45 - INFO - root -   epochs: 3/5, steps: 1300/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:27:52 - INFO - root -   epochs: 3/5, steps: 1400/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:29:49 - INFO - root -   epochs: 3/5, steps: 1500/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:31:53 - INFO - root -   epochs: 3/5, steps: 1600/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:32:53 - INFO - root -   val epoch 3: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:32:53 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-3 and ../trained/qwen-prefix-new/checkpoint-last
04/18/2025 07:34:35 - INFO - root -   epochs: 4/5, steps: 1700/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:36:41 - INFO - root -   epochs: 4/5, steps: 1800/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:38:40 - INFO - root -   epochs: 4/5, steps: 1900/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:40:38 - INFO - root -   epochs: 4/5, steps: 2000/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:42:49 - INFO - root -   epochs: 4/5, steps: 2100/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:44:42 - INFO - root -   val epoch 4: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:44:42 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-4 and ../trained/qwen-prefix-new/checkpoint-last
04/18/2025 07:45:35 - INFO - root -   epochs: 5/5, steps: 2200/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:47:35 - INFO - root -   epochs: 5/5, steps: 2300/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:49:43 - INFO - root -   epochs: 5/5, steps: 2400/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:51:56 - INFO - root -   epochs: 5/5, steps: 2500/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:53:56 - INFO - root -   epochs: 5/5, steps: 2600/2695, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:56:30 - INFO - root -   val epoch 5: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/18/2025 07:56:30 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-5 and ../trained/qwen-prefix-new/checkpoint-last
04/20/2025 09:40:26 - INFO - root -   Device: cuda, n_gpu: 2
04/20/2025 09:40:46 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/20/2025 09:40:46 - INFO - root -   ***** Running training *****
04/20/2025 09:40:46 - INFO - root -     Num samples = 886
04/20/2025 09:40:46 - INFO - root -     Num epoch = 5
04/20/2025 09:40:46 - INFO - root -     Batch size= 1
04/20/2025 09:40:46 - INFO - root -     Total batch size (w. accumulation) = 2
04/20/2025 09:40:46 - INFO - root -     Gradient Accumulation steps = 2
04/20/2025 09:40:46 - INFO - root -     Total optimization steps = 2215
04/20/2025 09:40:46 - INFO - root -     Num val samples = 99
04/20/2025 09:40:46 - INFO - root -     Num parameters = 1777875968
04/20/2025 09:40:46 - INFO - root -     Num trainable parameters = 1204224
04/20/2025 09:40:46 - INFO - root -     Fraction of trainable parameters = 0.0677
04/20/2025 09:43:19 - INFO - root -   epochs: 1/5, steps: 100/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 09:45:52 - INFO - root -   epochs: 1/5, steps: 200/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 09:48:27 - INFO - root -   epochs: 1/5, steps: 300/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 09:51:04 - INFO - root -   epochs: 1/5, steps: 400/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 09:52:49 - INFO - root -   val epoch 1: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 09:52:49 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-1 and ../trained/qwen-prefix-new/checkpoint-last
04/20/2025 09:54:15 - INFO - root -   epochs: 2/5, steps: 500/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 09:56:48 - INFO - root -   epochs: 2/5, steps: 600/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 09:59:23 - INFO - root -   epochs: 2/5, steps: 700/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:01:52 - INFO - root -   epochs: 2/5, steps: 800/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:04:51 - INFO - root -   val epoch 2: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:04:51 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-2 and ../trained/qwen-prefix-new/checkpoint-last
04/20/2025 10:05:10 - INFO - root -   epochs: 3/5, steps: 900/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:07:43 - INFO - root -   epochs: 3/5, steps: 1000/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:10:16 - INFO - root -   epochs: 3/5, steps: 1100/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:12:48 - INFO - root -   epochs: 3/5, steps: 1200/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:15:30 - INFO - root -   epochs: 3/5, steps: 1300/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:16:52 - INFO - root -   val epoch 3: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:16:52 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-3 and ../trained/qwen-prefix-new/checkpoint-last
04/20/2025 10:18:37 - INFO - root -   epochs: 4/5, steps: 1400/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:21:06 - INFO - root -   epochs: 4/5, steps: 1500/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:23:49 - INFO - root -   epochs: 4/5, steps: 1600/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:26:21 - INFO - root -   epochs: 4/5, steps: 1700/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:28:54 - INFO - root -   val epoch 4: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:28:54 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-4 and ../trained/qwen-prefix-new/checkpoint-last
04/20/2025 10:29:40 - INFO - root -   epochs: 5/5, steps: 1800/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:32:13 - INFO - root -   epochs: 5/5, steps: 1900/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:34:47 - INFO - root -   epochs: 5/5, steps: 2000/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:37:19 - INFO - root -   epochs: 5/5, steps: 2100/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:39:51 - INFO - root -   epochs: 5/5, steps: 2200/2215, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:40:56 - INFO - root -   val epoch 5: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/20/2025 10:40:56 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-5 and ../trained/qwen-prefix-new/checkpoint-last
04/21/2025 06:24:40 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 06:33:52 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 06:34:09 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/21/2025 06:34:09 - INFO - root -   ***** Running training *****
04/21/2025 06:34:09 - INFO - root -     Num samples = 885
04/21/2025 06:34:09 - INFO - root -     Num epoch = 5
04/21/2025 06:34:09 - INFO - root -     Batch size= 1
04/21/2025 06:34:09 - INFO - root -     Total batch size (w. accumulation) = 2
04/21/2025 06:34:09 - INFO - root -     Gradient Accumulation steps = 2
04/21/2025 06:34:09 - INFO - root -     Total optimization steps = 2210
04/21/2025 06:34:09 - INFO - root -     Num val samples = 99
04/21/2025 06:34:09 - INFO - root -     Num parameters = 1777875968
04/21/2025 06:34:09 - INFO - root -     Num trainable parameters = 1204224
04/21/2025 06:34:09 - INFO - root -     Fraction of trainable parameters = 0.0677
04/21/2025 06:36:12 - INFO - root -   epochs: 1/5, steps: 100/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 06:38:08 - INFO - root -   epochs: 1/5, steps: 200/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 06:40:14 - INFO - root -   epochs: 1/5, steps: 300/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 06:42:19 - INFO - root -   epochs: 1/5, steps: 400/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 06:43:45 - INFO - root -   val epoch 1: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 06:43:45 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-1 and ../trained/qwen-prefix-new/checkpoint-last
04/21/2025 06:44:54 - INFO - root -   epochs: 2/5, steps: 500/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 06:47:04 - INFO - root -   epochs: 2/5, steps: 600/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 06:49:03 - INFO - root -   epochs: 2/5, steps: 700/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 06:51:06 - INFO - root -   epochs: 2/5, steps: 800/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 06:53:20 - INFO - root -   val epoch 2: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 06:53:20 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-2 and ../trained/qwen-prefix-new/checkpoint-last
04/21/2025 06:53:39 - INFO - root -   epochs: 3/5, steps: 900/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 06:55:42 - INFO - root -   epochs: 3/5, steps: 1000/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 06:57:41 - INFO - root -   epochs: 3/5, steps: 1100/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 06:59:50 - INFO - root -   epochs: 3/5, steps: 1200/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 07:01:54 - INFO - root -   epochs: 3/5, steps: 1300/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 07:02:55 - INFO - root -   val epoch 3: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 07:02:55 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-3 and ../trained/qwen-prefix-new/checkpoint-last
04/21/2025 07:04:25 - INFO - root -   epochs: 4/5, steps: 1400/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 07:06:24 - INFO - root -   epochs: 4/5, steps: 1500/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 07:08:36 - INFO - root -   epochs: 4/5, steps: 1600/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 07:10:38 - INFO - root -   epochs: 4/5, steps: 1700/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 07:12:30 - INFO - root -   val epoch 4: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 07:12:30 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-4 and ../trained/qwen-prefix-new/checkpoint-last
04/21/2025 07:13:08 - INFO - root -   epochs: 5/5, steps: 1800/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 07:15:09 - INFO - root -   epochs: 5/5, steps: 1900/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 07:17:09 - INFO - root -   epochs: 5/5, steps: 2000/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 07:19:16 - INFO - root -   epochs: 5/5, steps: 2100/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 07:21:19 - INFO - root -   epochs: 5/5, steps: 2200/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 07:22:05 - INFO - root -   val epoch 5: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 07:22:05 - INFO - root -   Saving model checkpoint to ../trained/qwen-prefix-new/checkpoint-epoch-5 and ../trained/qwen-prefix-new/checkpoint-last
04/21/2025 07:53:34 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 07:53:51 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=1700, learning_rate=0.01, contrastive_loss_ratio=400, max_num_tokens=1024, grad_acc_steps=2, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/21/2025 07:53:51 - INFO - root -   ***** Running training *****
04/21/2025 07:53:51 - INFO - root -     Num samples            = 885
04/21/2025 07:53:51 - INFO - root -     Num epochs             = 5
04/21/2025 07:53:51 - INFO - root -     Total optimization steps = 2210
04/21/2025 08:04:44 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 08:05:01 - INFO - root -   Training on 885 examples
04/21/2025 08:24:40 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 08:24:57 - INFO - root -   Training on 885 examples
04/21/2025 13:04:03 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 13:04:20 - INFO - root -   Training on 885 examples
04/21/2025 13:26:53 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 13:27:09 - INFO - root -   Training on 885 examples
04/21/2025 13:55:08 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 13:55:25 - INFO - root -   Training on 885 examples
04/21/2025 14:07:19 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 14:07:32 - INFO - root -   Training on 885 examples
04/21/2025 14:10:07 - INFO - root -   Epoch 1/5, Step 100/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 14:12:35 - INFO - root -   Epoch 1/5, Step 200/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 14:15:15 - INFO - root -   Epoch 1/5, Step 300/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 14:17:54 - INFO - root -   Epoch 1/5, Step 400/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 14:19:44 - INFO - root -   Validation after epoch 1: lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 14:21:10 - INFO - root -   Epoch 2/5, Step 500/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 14:23:51 - INFO - root -   Epoch 2/5, Step 600/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 14:26:24 - INFO - root -   Epoch 2/5, Step 700/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 14:27:45 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 14:27:57 - INFO - root -   Training on 885 examples
04/21/2025 14:28:49 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 14:29:01 - INFO - root -   Training on 885 examples
04/21/2025 14:31:36 - INFO - root -   Epoch 1/5, Step 100/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 14:34:03 - INFO - root -   Epoch 1/5, Step 200/2210, lm_loss: nan, contrastive_loss: nan, kl_loss: nan, loss: nan
04/21/2025 15:50:58 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 15:51:10 - INFO - root -   Training on 885 examples
04/21/2025 15:51:52 - INFO - root -   Epoch 1/5, Step 100/1105, skipped: 1.0
04/21/2025 15:52:35 - INFO - root -   Epoch 1/5, Step 200/1105, skipped: 1.0
04/21/2025 15:52:54 - INFO - root -   Validation after epoch 1: skipped: 1.0
04/21/2025 15:53:29 - INFO - root -   Epoch 2/5, Step 300/1105, skipped: 1.0025
04/21/2025 15:54:11 - INFO - root -   Epoch 2/5, Step 400/1105, skipped: 1.0
04/21/2025 15:54:38 - INFO - root -   Validation after epoch 2: skipped: 1.0
04/21/2025 15:55:04 - INFO - root -   Epoch 3/5, Step 500/1105, skipped: 1.0025
04/21/2025 15:55:46 - INFO - root -   Epoch 3/5, Step 600/1105, skipped: 1.0
04/21/2025 15:56:23 - INFO - root -   Validation after epoch 3: skipped: 1.0
04/21/2025 15:56:39 - INFO - root -   Epoch 4/5, Step 700/1105, skipped: 1.0025
04/21/2025 15:57:21 - INFO - root -   Epoch 4/5, Step 800/1105, skipped: 1.0
04/21/2025 15:58:06 - INFO - root -   Validation after epoch 4: skipped: 1.0
04/21/2025 15:58:14 - INFO - root -   Epoch 5/5, Step 900/1105, skipped: 1.0025
04/21/2025 15:58:57 - INFO - root -   Epoch 5/5, Step 1000/1105, skipped: 1.0
04/21/2025 16:02:11 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 16:02:29 - INFO - root -   Training on 936 examples
04/21/2025 16:03:32 - INFO - root -   Epoch 1/5, Step 100/1170, skipped: 1.0
04/21/2025 16:04:34 - INFO - root -   Epoch 1/5, Step 200/1170, skipped: 1.0
04/21/2025 16:05:09 - INFO - root -   Validation after epoch 1: skipped: 1.0
04/21/2025 16:05:50 - INFO - root -   Epoch 2/5, Step 300/1170, skipped: 1.0
04/21/2025 16:06:52 - INFO - root -   Epoch 2/5, Step 400/1170, skipped: 1.0
04/21/2025 16:07:48 - INFO - root -   Validation after epoch 2: skipped: 1.0
04/21/2025 16:08:08 - INFO - root -   Epoch 3/5, Step 500/1170, skipped: 1.0
04/21/2025 16:09:10 - INFO - root -   Epoch 3/5, Step 600/1170, skipped: 1.0
04/21/2025 16:10:10 - INFO - root -   Epoch 3/5, Step 700/1170, skipped: 1.0
04/21/2025 16:10:26 - INFO - root -   Validation after epoch 3: skipped: 1.0
04/21/2025 16:11:26 - INFO - root -   Epoch 4/5, Step 800/1170, skipped: 1.0
04/21/2025 16:12:28 - INFO - root -   Epoch 4/5, Step 900/1170, skipped: 1.0
04/21/2025 16:13:04 - INFO - root -   Validation after epoch 4: skipped: 1.0
04/21/2025 16:13:44 - INFO - root -   Epoch 5/5, Step 1000/1170, skipped: 1.0
04/21/2025 16:14:46 - INFO - root -   Epoch 5/5, Step 1100/1170, skipped: 1.0
04/21/2025 16:15:42 - INFO - root -   Validation after epoch 5: skipped: 1.0
04/21/2025 16:36:19 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 16:36:31 - INFO - root -   Training on 885 examples
04/21/2025 16:37:13 - INFO - root -   Epoch 1/5, Step 100/1105, skipped: 1.0
04/21/2025 16:37:56 - INFO - root -   Epoch 1/5, Step 200/1105, skipped: 1.0
04/21/2025 16:38:15 - INFO - root -   Validation after epoch 1: skipped: 1.0
04/21/2025 16:38:50 - INFO - root -   Epoch 2/5, Step 300/1105, skipped: 1.0025
04/21/2025 16:39:32 - INFO - root -   Epoch 2/5, Step 400/1105, skipped: 1.0
04/21/2025 16:39:59 - INFO - root -   Validation after epoch 2: skipped: 1.0
04/21/2025 16:40:25 - INFO - root -   Epoch 3/5, Step 500/1105, skipped: 1.0025
04/21/2025 16:41:07 - INFO - root -   Epoch 3/5, Step 600/1105, skipped: 1.0
04/21/2025 16:41:43 - INFO - root -   Validation after epoch 3: skipped: 1.0
04/21/2025 16:42:00 - INFO - root -   Epoch 4/5, Step 700/1105, skipped: 1.0025
04/21/2025 16:42:42 - INFO - root -   Epoch 4/5, Step 800/1105, skipped: 1.0
04/21/2025 16:43:28 - INFO - root -   Validation after epoch 4: skipped: 1.0
04/21/2025 16:43:35 - INFO - root -   Epoch 5/5, Step 900/1105, skipped: 1.0025
04/21/2025 16:44:18 - INFO - root -   Epoch 5/5, Step 1000/1105, skipped: 1.0
04/21/2025 16:44:59 - INFO - root -   Epoch 5/5, Step 1100/1105, skipped: 1.0
04/21/2025 16:45:12 - INFO - root -   Validation after epoch 5: skipped: 1.0
04/21/2025 23:50:33 - INFO - root -   Device: cuda, n_gpu: 2
04/21/2025 23:50:45 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/21/2025 23:50:45 - INFO - root -   ***** Running training *****
04/21/2025 23:50:45 - INFO - root -     Num samples = 885
04/21/2025 23:50:45 - INFO - root -     Num epoch = 5
04/21/2025 23:50:45 - INFO - root -     Batch size= 1
04/21/2025 23:50:45 - INFO - root -     Total batch size (w. accumulation) = 4
04/21/2025 23:50:45 - INFO - root -     Gradient Accumulation steps = 4
04/21/2025 23:50:45 - INFO - root -     Total optimization steps = 1105
04/21/2025 23:50:45 - INFO - root -     Num val samples = 99
04/21/2025 23:50:45 - INFO - root -     Num parameters = 1777875968
04/21/2025 23:50:45 - INFO - root -     Num trainable parameters = 1204224
04/21/2025 23:50:45 - INFO - root -     Fraction of trainable parameters = 0.0677
04/22/2025 07:38:29 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 07:38:40 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 07:38:41 - INFO - root -   ***** Running training *****
04/22/2025 07:38:41 - INFO - root -     Num samples = 885
04/22/2025 07:38:41 - INFO - root -     Num epoch = 5
04/22/2025 07:38:41 - INFO - root -     Batch size= 1
04/22/2025 07:38:41 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 07:38:41 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 07:38:41 - INFO - root -     Total optimization steps = 1105
04/22/2025 07:38:41 - INFO - root -     Num val samples = 99
04/22/2025 07:38:41 - INFO - root -     Num parameters = 1777875968
04/22/2025 07:38:41 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 07:38:41 - INFO - root -     Fraction of trainable parameters = 0.0677
04/22/2025 07:46:19 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 07:46:31 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 07:46:31 - INFO - root -   ***** Running training *****
04/22/2025 07:46:31 - INFO - root -     Num samples = 885
04/22/2025 07:46:31 - INFO - root -     Num epoch = 5
04/22/2025 07:46:31 - INFO - root -     Batch size= 1
04/22/2025 07:46:31 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 07:46:31 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 07:46:31 - INFO - root -     Total optimization steps = 1105
04/22/2025 07:46:31 - INFO - root -     Num val samples = 99
04/22/2025 07:46:31 - INFO - root -     Num parameters = 1777875968
04/22/2025 07:46:31 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 07:46:31 - INFO - root -     Fraction of trainable parameters = 0.0677
04/22/2025 08:19:27 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 08:19:38 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 08:19:38 - INFO - root -   ***** Running training *****
04/22/2025 08:19:38 - INFO - root -     Num samples = 885
04/22/2025 08:19:38 - INFO - root -     Num epoch = 5
04/22/2025 08:19:38 - INFO - root -     Batch size= 1
04/22/2025 08:19:38 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 08:19:38 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 08:19:38 - INFO - root -     Total optimization steps = 1105
04/22/2025 08:19:38 - INFO - root -     Num val samples = 99
04/22/2025 08:19:38 - INFO - root -     Num parameters = 1777875968
04/22/2025 08:19:38 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 08:19:38 - INFO - root -     Fraction of trainable parameters = 0.0677
04/22/2025 10:32:54 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 10:33:12 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 10:33:12 - INFO - root -   ***** Running training *****
04/22/2025 10:33:12 - INFO - root -     Num samples = 885
04/22/2025 10:33:12 - INFO - root -     Num epoch = 5
04/22/2025 10:33:12 - INFO - root -     Batch size= 1
04/22/2025 10:33:12 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 10:33:12 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 10:33:12 - INFO - root -     Total optimization steps = 1105
04/22/2025 10:33:12 - INFO - root -     Num val samples = 99
04/22/2025 10:33:12 - INFO - root -     Num parameters = 1777875968
04/22/2025 10:33:12 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 10:33:12 - INFO - root -     Fraction of trainable parameters = 0.0677
04/22/2025 10:42:35 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 10:42:53 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 10:42:53 - INFO - root -   ***** Running training *****
04/22/2025 10:42:53 - INFO - root -     Num samples = 885
04/22/2025 10:42:53 - INFO - root -     Num epoch = 5
04/22/2025 10:42:53 - INFO - root -     Batch size= 1
04/22/2025 10:42:53 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 10:42:53 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 10:42:53 - INFO - root -     Total optimization steps = 1105
04/22/2025 10:42:53 - INFO - root -     Num val samples = 99
04/22/2025 10:42:53 - INFO - root -     Num parameters = 1777875968
04/22/2025 10:42:53 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 10:42:53 - INFO - root -     Fraction of trainable parameters = 0.0677
04/22/2025 10:45:10 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 10:45:28 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 10:45:28 - INFO - root -   ***** Running training *****
04/22/2025 10:45:28 - INFO - root -     Num samples = 885
04/22/2025 10:45:28 - INFO - root -     Num epoch = 5
04/22/2025 10:45:28 - INFO - root -     Batch size= 1
04/22/2025 10:45:28 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 10:45:28 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 10:45:28 - INFO - root -     Total optimization steps = 1105
04/22/2025 10:45:28 - INFO - root -     Num val samples = 99
04/22/2025 10:45:28 - INFO - root -     Num parameters = 1777875968
04/22/2025 10:45:28 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 10:45:28 - INFO - root -     Fraction of trainable parameters = 0.0677
04/22/2025 10:54:39 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 10:54:58 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 10:54:58 - INFO - root -   ***** Running training *****
04/22/2025 10:54:58 - INFO - root -     Num samples = 885
04/22/2025 10:54:58 - INFO - root -     Num epoch = 5
04/22/2025 10:54:58 - INFO - root -     Batch size= 1
04/22/2025 10:54:58 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 10:54:58 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 10:54:58 - INFO - root -     Total optimization steps = 1105
04/22/2025 10:54:58 - INFO - root -     Num val samples = 99
04/22/2025 10:54:58 - INFO - root -     Num parameters = 1777875968
04/22/2025 10:54:58 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 10:54:58 - INFO - root -     Fraction of trainable parameters = 0.0677
04/22/2025 11:03:13 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 11:03:32 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 11:03:32 - INFO - root -   ***** Running training *****
04/22/2025 11:03:32 - INFO - root -     Num samples = 885
04/22/2025 11:03:32 - INFO - root -     Num epoch = 5
04/22/2025 11:03:32 - INFO - root -     Batch size= 1
04/22/2025 11:03:32 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 11:03:32 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 11:03:32 - INFO - root -     Total optimization steps = 1105
04/22/2025 11:03:32 - INFO - root -     Num val samples = 99
04/22/2025 11:03:32 - INFO - root -     Num parameters = 1777875968
04/22/2025 11:03:32 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 11:03:32 - INFO - root -     Fraction of trainable parameters = 0.0677
04/22/2025 11:12:15 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 11:12:33 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 11:12:33 - INFO - root -   ***** Running training *****
04/22/2025 11:12:33 - INFO - root -     Num samples = 885
04/22/2025 11:12:33 - INFO - root -     Num epoch = 5
04/22/2025 11:12:33 - INFO - root -     Batch size= 1
04/22/2025 11:12:33 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 11:12:33 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 11:12:33 - INFO - root -     Total optimization steps = 1105
04/22/2025 11:12:33 - INFO - root -     Num val samples = 99
04/22/2025 11:12:33 - INFO - root -     Num parameters = 1777875968
04/22/2025 11:12:33 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 11:12:33 - INFO - root -     Fraction of trainable parameters = 0.0677
04/22/2025 11:13:30 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 11:13:48 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 11:13:48 - INFO - root -   ***** Running training *****
04/22/2025 11:13:48 - INFO - root -     Num samples = 885
04/22/2025 11:13:48 - INFO - root -     Num epoch = 5
04/22/2025 11:13:48 - INFO - root -     Batch size= 1
04/22/2025 11:13:48 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 11:13:48 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 11:13:48 - INFO - root -     Total optimization steps = 1105
04/22/2025 11:13:48 - INFO - root -     Num val samples = 99
04/22/2025 11:13:48 - INFO - root -     Num parameters = 1777875968
04/22/2025 11:13:48 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 11:13:48 - INFO - root -     Fraction of trainable parameters = 0.0677
04/22/2025 11:15:31 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 11:15:49 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 11:15:49 - INFO - root -   ***** Running training *****
04/22/2025 11:15:49 - INFO - root -     Num samples = 885
04/22/2025 11:15:49 - INFO - root -     Num epoch = 5
04/22/2025 11:15:49 - INFO - root -     Batch size= 1
04/22/2025 11:15:49 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 11:15:49 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 11:15:49 - INFO - root -     Total optimization steps = 1105
04/22/2025 11:15:49 - INFO - root -     Num val samples = 99
04/22/2025 11:15:49 - INFO - root -     Num parameters = 1777875968
04/22/2025 11:15:49 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 11:15:49 - INFO - root -     Fraction of trainable parameters = 0.0677
04/22/2025 11:21:01 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 11:21:19 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 11:21:19 - INFO - root -   ***** Running training *****
04/22/2025 11:21:19 - INFO - root -     Num samples = 885
04/22/2025 11:21:19 - INFO - root -     Num epoch = 5
04/22/2025 11:21:19 - INFO - root -     Batch size= 1
04/22/2025 11:21:19 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 11:21:19 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 11:21:19 - INFO - root -     Total optimization steps = 1105
04/22/2025 11:21:19 - INFO - root -     Num val samples = 99
04/22/2025 11:21:19 - INFO - root -     Num parameters = 1777875968
04/22/2025 11:21:19 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 11:21:19 - INFO - root -     Fraction of trainable parameters = 0.0677
04/22/2025 15:46:24 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 15:46:40 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 15:46:40 - INFO - root -   ***** Running training *****
04/22/2025 15:46:40 - INFO - root -     Num samples = 885
04/22/2025 15:46:40 - INFO - root -     Num epoch = 5
04/22/2025 15:46:40 - INFO - root -     Batch size= 1
04/22/2025 15:46:40 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 15:46:40 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 15:46:40 - INFO - root -     Total optimization steps = 1105
04/22/2025 15:46:40 - INFO - root -     Num val samples = 99
04/22/2025 15:46:40 - INFO - root -     Num parameters = 1777875968
04/22/2025 15:46:40 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 15:46:40 - INFO - root -     Fraction of trainable parameters = 0.0677
04/22/2025 15:56:24 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 16:02:22 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 16:05:28 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 16:09:40 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 16:09:57 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 16:09:57 - INFO - root -   ***** Running training *****
04/22/2025 16:09:57 - INFO - root -     Num samples = 885
04/22/2025 16:09:57 - INFO - root -     Num epoch = 5
04/22/2025 16:09:57 - INFO - root -     Batch size= 1
04/22/2025 16:09:57 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 16:09:57 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 16:09:57 - INFO - root -     Total optimization steps = 1105
04/22/2025 16:09:57 - INFO - root -     Num val samples = 99
04/22/2025 16:09:57 - INFO - root -     Num parameters = 1544502272
04/22/2025 16:09:57 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 16:09:57 - INFO - root -     Fraction of trainable parameters = 0.078
04/22/2025 16:23:09 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 16:23:26 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 16:23:26 - INFO - root -   ***** Running training *****
04/22/2025 16:23:26 - INFO - root -     Num samples = 885
04/22/2025 16:23:26 - INFO - root -     Num epoch = 5
04/22/2025 16:23:26 - INFO - root -     Batch size= 1
04/22/2025 16:23:26 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 16:23:26 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 16:23:26 - INFO - root -     Total optimization steps = 1105
04/22/2025 16:23:26 - INFO - root -     Num val samples = 99
04/22/2025 16:23:26 - INFO - root -     Num parameters = 1544502272
04/22/2025 16:23:26 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 16:23:26 - INFO - root -     Fraction of trainable parameters = 0.078
04/22/2025 18:20:13 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 18:20:28 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 18:20:28 - INFO - root -   ***** Running training *****
04/22/2025 18:20:28 - INFO - root -     Num samples = 885
04/22/2025 18:20:28 - INFO - root -     Num epoch = 5
04/22/2025 18:20:28 - INFO - root -     Batch size= 1
04/22/2025 18:20:28 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 18:20:28 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 18:20:28 - INFO - root -     Total optimization steps = 1105
04/22/2025 18:20:28 - INFO - root -     Num val samples = 99
04/22/2025 18:20:28 - INFO - root -     Num parameters = 1544502272
04/22/2025 18:20:28 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 18:20:28 - INFO - root -     Fraction of trainable parameters = 0.078
04/22/2025 18:28:17 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 18:28:27 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 18:28:27 - INFO - root -   ***** Running training *****
04/22/2025 18:28:27 - INFO - root -     Num samples = 885
04/22/2025 18:28:27 - INFO - root -     Num epoch = 5
04/22/2025 18:28:27 - INFO - root -     Batch size= 1
04/22/2025 18:28:27 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 18:28:27 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 18:28:27 - INFO - root -     Total optimization steps = 1105
04/22/2025 18:28:27 - INFO - root -     Num val samples = 99
04/22/2025 18:28:27 - INFO - root -     Num parameters = 1544502272
04/22/2025 18:28:27 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 18:28:27 - INFO - root -     Fraction of trainable parameters = 0.078
04/22/2025 18:33:33 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 18:33:50 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 18:33:50 - INFO - root -   ***** Running training *****
04/22/2025 18:33:50 - INFO - root -     Num samples = 885
04/22/2025 18:33:50 - INFO - root -     Num epoch = 5
04/22/2025 18:33:50 - INFO - root -     Batch size= 1
04/22/2025 18:33:50 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 18:33:50 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 18:33:50 - INFO - root -     Total optimization steps = 1105
04/22/2025 18:33:50 - INFO - root -     Num val samples = 99
04/22/2025 18:33:50 - INFO - root -     Num parameters = 1544502272
04/22/2025 18:33:50 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 18:33:50 - INFO - root -     Fraction of trainable parameters = 0.078
04/22/2025 19:00:21 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 19:00:39 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 19:00:39 - INFO - root -   ***** Running training *****
04/22/2025 19:00:39 - INFO - root -     Num samples = 885
04/22/2025 19:00:39 - INFO - root -     Num epoch = 5
04/22/2025 19:00:39 - INFO - root -     Batch size= 1
04/22/2025 19:00:39 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 19:00:39 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 19:00:39 - INFO - root -     Total optimization steps = 1105
04/22/2025 19:00:39 - INFO - root -     Num val samples = 99
04/22/2025 19:00:39 - INFO - root -     Num parameters = 1544502272
04/22/2025 19:00:39 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 19:00:39 - INFO - root -     Fraction of trainable parameters = 0.078
04/22/2025 19:07:25 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 19:07:42 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 19:11:34 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 19:11:52 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 19:11:52 - INFO - root -   ***** Running training *****
04/22/2025 19:11:52 - INFO - root -     Num samples = 885
04/22/2025 19:11:52 - INFO - root -     Num epoch = 5
04/22/2025 19:11:52 - INFO - root -     Batch size= 1
04/22/2025 19:11:52 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 19:11:52 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 19:11:52 - INFO - root -     Total optimization steps = 1105
04/22/2025 19:11:52 - INFO - root -     Num val samples = 99
04/22/2025 19:11:52 - INFO - root -     Num parameters = 1544502272
04/22/2025 19:11:52 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 19:11:52 - INFO - root -     Fraction of trainable parameters = 0.078
04/22/2025 19:15:15 - INFO - root -   Device: cuda, n_gpu: 2
04/22/2025 19:15:32 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/22/2025 19:15:32 - INFO - root -   ***** Running training *****
04/22/2025 19:15:32 - INFO - root -     Num samples = 885
04/22/2025 19:15:32 - INFO - root -     Num epoch = 5
04/22/2025 19:15:32 - INFO - root -     Batch size= 1
04/22/2025 19:15:32 - INFO - root -     Total batch size (w. accumulation) = 4
04/22/2025 19:15:32 - INFO - root -     Gradient Accumulation steps = 4
04/22/2025 19:15:32 - INFO - root -     Total optimization steps = 1105
04/22/2025 19:15:32 - INFO - root -     Num val samples = 99
04/22/2025 19:15:32 - INFO - root -     Num parameters = 1544502272
04/22/2025 19:15:32 - INFO - root -     Num trainable parameters = 1204224
04/22/2025 19:15:32 - INFO - root -     Fraction of trainable parameters = 0.078
04/23/2025 00:12:53 - INFO - root -   Device: cuda, n_gpu: 2
04/23/2025 00:13:10 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/23/2025 00:13:10 - INFO - root -   ***** Running training *****
04/23/2025 00:13:10 - INFO - root -     Num samples = 885
04/23/2025 00:13:10 - INFO - root -     Num epoch = 5
04/23/2025 00:13:10 - INFO - root -     Batch size= 1
04/23/2025 00:13:10 - INFO - root -     Total batch size (w. accumulation) = 4
04/23/2025 00:13:10 - INFO - root -     Gradient Accumulation steps = 4
04/23/2025 00:13:10 - INFO - root -     Total optimization steps = 1105
04/23/2025 00:13:10 - INFO - root -     Num val samples = 99
04/23/2025 00:13:10 - INFO - root -     Num parameters = 1544502272
04/23/2025 00:13:10 - INFO - root -     Num trainable parameters = 1204224
04/23/2025 00:13:10 - INFO - root -     Fraction of trainable parameters = 0.078
04/23/2025 00:25:37 - INFO - root -   Device: cuda, n_gpu: 2
04/23/2025 00:47:00 - INFO - root -   Device: cuda, n_gpu: 2
04/23/2025 00:47:11 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/23/2025 00:47:11 - INFO - root -   ***** Running training *****
04/23/2025 00:47:11 - INFO - root -     Num samples = 885
04/23/2025 00:47:11 - INFO - root -     Num epoch = 5
04/23/2025 00:47:11 - INFO - root -     Batch size= 1
04/23/2025 00:47:11 - INFO - root -     Total batch size (w. accumulation) = 4
04/23/2025 00:47:11 - INFO - root -     Gradient Accumulation steps = 4
04/23/2025 00:47:11 - INFO - root -     Total optimization steps = 1105
04/23/2025 00:47:11 - INFO - root -     Num val samples = 99
04/23/2025 00:47:11 - INFO - root -     Num parameters = 1544502272
04/23/2025 00:47:11 - INFO - root -     Num trainable parameters = 1204224
04/23/2025 00:47:11 - INFO - root -     Fraction of trainable parameters = 0.078
04/23/2025 07:43:13 - INFO - root -   Device: cuda, n_gpu: 2
04/23/2025 07:43:28 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/23/2025 07:43:28 - INFO - root -   ***** Running training *****
04/23/2025 07:43:28 - INFO - root -     Num samples = 885
04/23/2025 07:43:28 - INFO - root -     Num epoch = 5
04/23/2025 07:43:28 - INFO - root -     Batch size= 1
04/23/2025 07:43:28 - INFO - root -     Total batch size (w. accumulation) = 4
04/23/2025 07:43:28 - INFO - root -     Gradient Accumulation steps = 4
04/23/2025 07:43:28 - INFO - root -     Total optimization steps = 1105
04/23/2025 07:43:28 - INFO - root -     Num val samples = 99
04/23/2025 07:43:28 - INFO - root -     Num parameters = 1544502272
04/23/2025 07:43:28 - INFO - root -     Num trainable parameters = 1204224
04/23/2025 07:43:28 - INFO - root -     Fraction of trainable parameters = 0.078
04/23/2025 10:35:59 - INFO - root -   Device: cuda, n_gpu: 2
04/23/2025 10:36:09 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/23/2025 10:36:09 - INFO - root -   ***** Running training *****
04/23/2025 10:36:09 - INFO - root -     Num samples = 885
04/23/2025 10:36:09 - INFO - root -     Num epoch = 5
04/23/2025 10:36:09 - INFO - root -     Batch size= 1
04/23/2025 10:36:09 - INFO - root -     Total batch size (w. accumulation) = 4
04/23/2025 10:36:09 - INFO - root -     Gradient Accumulation steps = 4
04/23/2025 10:36:09 - INFO - root -     Total optimization steps = 1105
04/23/2025 10:36:09 - INFO - root -     Num val samples = 99
04/23/2025 10:36:09 - INFO - root -     Num parameters = 1544502272
04/23/2025 10:36:09 - INFO - root -     Num trainable parameters = 1204224
04/23/2025 10:36:09 - INFO - root -     Fraction of trainable parameters = 0.078
04/23/2025 10:38:15 - INFO - root -   Device: cuda, n_gpu: 2
04/23/2025 10:38:25 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/23/2025 10:38:25 - INFO - root -   ***** Running training *****
04/23/2025 10:38:25 - INFO - root -     Num samples = 885
04/23/2025 10:38:25 - INFO - root -     Num epoch = 5
04/23/2025 10:38:25 - INFO - root -     Batch size= 1
04/23/2025 10:38:25 - INFO - root -     Total batch size (w. accumulation) = 4
04/23/2025 10:38:25 - INFO - root -     Gradient Accumulation steps = 4
04/23/2025 10:38:25 - INFO - root -     Total optimization steps = 1105
04/23/2025 10:38:25 - INFO - root -     Num val samples = 99
04/23/2025 10:38:25 - INFO - root -     Num parameters = 1544502272
04/23/2025 10:38:25 - INFO - root -     Num trainable parameters = 1204224
04/23/2025 10:38:25 - INFO - root -     Fraction of trainable parameters = 0.078
04/23/2025 15:32:45 - INFO - root -   Device: cuda, n_gpu: 2
04/23/2025 17:05:50 - INFO - root -   Device: cuda, n_gpu: 2
04/23/2025 17:06:40 - INFO - root -   Device: cuda, n_gpu: 2
04/24/2025 22:34:27 - INFO - root -   Device: cuda, n_gpu: 2
04/24/2025 22:37:27 - INFO - root -   Device: cuda, n_gpu: 2
04/25/2025 05:25:39 - INFO - root -   Device: cuda, n_gpu: 2
04/25/2025 09:36:26 - INFO - root -   Device: cuda, n_gpu: 2
04/25/2025 09:36:51 - INFO - root -   Training on 1440 examples
04/25/2025 09:48:10 - INFO - root -   Device: cuda, n_gpu: 2
04/25/2025 09:48:33 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/25/2025 09:48:33 - INFO - root -   ***** Running training *****
04/25/2025 09:48:33 - INFO - root -     Num samples = 1440
04/25/2025 09:48:33 - INFO - root -     Total optimization steps = 1800
04/27/2025 12:05:59 - INFO - root -   Device: cuda, n_gpu: 2
04/27/2025 12:06:17 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/27/2025 12:06:17 - INFO - root -   ***** Running training *****
04/27/2025 12:06:17 - INFO - root -     Num samples = 1440
04/27/2025 12:06:17 - INFO - root -     Num epochs  = 5
04/27/2025 12:06:17 - INFO - root -     Grad steps  = 4
04/27/2025 12:06:17 - INFO - root -     Total steps = 1800
04/27/2025 12:06:17 - INFO - root -     Trainable   = 1204224/1544502272 params
04/27/2025 12:13:37 - INFO - root -   Device: cuda, n_gpu: 2
04/27/2025 12:13:55 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/27/2025 12:13:55 - INFO - root -   ***** Running training *****
04/27/2025 12:13:55 - INFO - root -     Num samples               = 1440
04/27/2025 12:13:55 - INFO - root -     Num epochs                = 5
04/27/2025 12:13:55 - INFO - root -     Grad accumulation steps   = 4
04/27/2025 12:13:55 - INFO - root -     Total optimization steps  = 1800
04/27/2025 12:13:55 - INFO - root -     Trainable parameters      = 1204224/1544502272
04/27/2025 12:58:28 - INFO - root -   Device: cuda, n_gpu: 2
04/27/2025 12:58:46 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/27/2025 12:58:46 - INFO - root -   ***** Running training *****
04/27/2025 12:58:46 - INFO - root -     Num samples             = 1440
04/27/2025 12:58:46 - INFO - root -     Num epochs              = 5
04/27/2025 12:58:46 - INFO - root -     Grad accumulation steps = 4
04/27/2025 12:58:46 - INFO - root -     Total optimization steps= 1800
04/27/2025 12:58:46 - INFO - root -     Trainable params        = 1204224/1544502272
04/27/2025 13:10:27 - INFO - root -   Device: cuda, n_gpu: 2
04/27/2025 13:21:48 - INFO - root -   Device: cuda, n_gpu: 2
04/28/2025 06:08:26 - INFO - root -   Device: cuda, n_gpu: 2
04/28/2025 06:09:53 - INFO - root -   Device: cuda, n_gpu: 2
04/28/2025 06:32:08 - INFO - root -   Device: cuda, n_gpu: 2
04/28/2025 06:36:36 - INFO - root -   Device: cuda, n_gpu: 2
04/28/2025 10:04:46 - INFO - root -   Device: cuda, n_gpu: 2
04/28/2025 10:30:34 - INFO - root -   Device: cuda, n_gpu: 2
04/28/2025 10:30:51 - INFO - root -   Device map: {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], 1: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]}
04/28/2025 10:30:51 - INFO - root -   Training on 1440 samples
04/28/2025 10:40:54 - INFO - root -   Device: cuda, n_gpu: 2
04/28/2025 10:41:17 - INFO - root -   Device map: {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], 1: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]}
04/28/2025 10:41:17 - INFO - root -   Training on 1440 samples
04/28/2025 11:00:15 - INFO - root -   Device: cuda, n_gpu: 2
04/28/2025 11:00:29 - INFO - root -   Device map: {0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], 1: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]}
04/28/2025 11:00:29 - INFO - root -   Training on 1440 samples
04/28/2025 11:28:31 - INFO - root -   Device: cuda, n_gpu: 2
04/28/2025 11:28:42 - INFO - root -   Training args Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
04/28/2025 11:28:42 - INFO - root -   ***** Running training *****
04/28/2025 11:28:42 - INFO - root -     Num samples = 1440
04/28/2025 11:28:42 - INFO - root -     Num epoch = 5
04/28/2025 11:28:42 - INFO - root -     Batch size = 1
04/28/2025 11:28:42 - INFO - root -     Total batch size (w. accumulation) = 4
04/28/2025 11:28:42 - INFO - root -     Gradient Accumulation steps = 4
04/28/2025 11:28:42 - INFO - root -     Total optimization steps = 1800
04/28/2025 11:38:22 - INFO - root -   Device: cuda, n_gpu: 2
04/28/2025 11:38:42 - INFO - root -   Training args: Namespace(output_name='qwen-prefix-new', data_dir='../data_train_val', output_dir='../trained/qwen-prefix-new', model_type='prefix', pretrain_dir='Qwen/Qwen2.5-Coder-1.5B-Instruct', vul_type=None, n_prefix_token=7, num_train_epochs=5, kl_loss_ratio=0, learning_rate=0.05, contrastive_loss_ratio=0, max_num_tokens=1024, grad_acc_steps=4, weight_decay=0.01, adam_epsilon=1e-08, warmup_steps=0, max_grad_norm=1.0, dropout=0.1, diff_level='mix', lm_loss_ratio=1, logging_steps=100, save_epochs=1, seed=1, logger=<RootLogger root (INFO)>, n_gpu=2, device=device(type='cuda'))
